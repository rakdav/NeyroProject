{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-14T12:02:04.720233Z",
     "start_time": "2025-02-14T12:02:04.678106Z"
    }
   },
   "source": [
    "from keras import preprocessing,Sequential, layers\n",
    "from keras.src.layers import Conv2D, MaxPooling2D, Flatten, Dropout,Dense\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:02:07.258533Z",
     "start_time": "2025-02-14T12:02:07.249840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir='train'\n",
    "val_dir='val'\n",
    "test_dir='test'\n",
    "img_width, img_height = 150, 150\n",
    "input_shape = (img_width, img_height, 3)\n",
    "epochs=30\n",
    "batch_size=16\n",
    "nb_train_samples=17500\n",
    "nb_validation_samples=3750\n",
    "nb_test_samples=3750"
   ],
   "id": "9276007d5e9d39a9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:02:11.353474Z",
     "start_time": "2025-02-14T12:02:11.087150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ],
   "id": "3b47e0c9e54e0c5d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:02:14.071257Z",
     "start_time": "2025-02-14T12:02:14.056780Z"
    }
   },
   "cell_type": "code",
   "source": "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
   "id": "d25ec81b72420b18",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:02:15.541674Z",
     "start_time": "2025-02-14T12:02:15.530403Z"
    }
   },
   "cell_type": "code",
   "source": "datagen=ImageDataGenerator(rescale=1./255)",
   "id": "d556385797fe6ad3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:03:10.624267Z",
     "start_time": "2025-02-14T12:03:09.587314Z"
    }
   },
   "cell_type": "code",
   "source": "train_generator=datagen.flow_from_directory(train_dir,target_size=(img_width, img_height),batch_size=batch_size,class_mode='binary')",
   "id": "56ce78e5b54b3cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:04:18.607344Z",
     "start_time": "2025-02-14T12:04:18.394249Z"
    }
   },
   "cell_type": "code",
   "source": "val_generator=datagen.flow_from_directory(val_dir,target_size=(img_width, img_height),batch_size=batch_size,class_mode='binary')",
   "id": "8480d78cb528f473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3750 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T12:04:50.803435Z",
     "start_time": "2025-02-14T12:04:50.676701Z"
    }
   },
   "cell_type": "code",
   "source": "test_generator=datagen.flow_from_directory(test_dir,target_size=(img_width, img_height),batch_size=batch_size,class_mode='binary')",
   "id": "34cd4026e3e3a235",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3750 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-14T12:09:16.631411Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(train_generator,steps_per_epoch=nb_train_samples//batch_size,epochs=epochs,validation_data=val_generator,validation_steps=nb_validation_samples//batch_size)",
   "id": "1f6419a5eb9978a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\administrator\\tensorflow\\pythonProject1\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m 143/1093\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m4:43\u001B[0m 298ms/step - accuracy: 0.5224 - loss: 0.7065"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
